{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\PROJECTS\\\\ML\\\\Brain-Tumor-Image-Classfication\\\\notebook'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\PROJECTS\\ML\\Brain-Tumor-Image-Classfication\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reza\\miniconda3\\envs\\brainMRI\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\PROJECTS\\\\ML\\\\Brain-Tumor-Image-Classfication'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "from brainMRI.logging import logger\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class PrepareDatasets:\n",
    "    data_dir:Path\n",
    "    save_dir: Path\n",
    "    validation_split: float\n",
    "    image_size: tuple[int, int]\n",
    "    batch_size: int\n",
    "    labels: str\n",
    "    subset: str\n",
    "    seed: int\n",
    "\n",
    "    def prepare_datasets(self):\n",
    "        \"\"\"\n",
    "        Prepare the training and validation datasets for the machine learning model.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[tf.data.Dataset, tf.data.Dataset]: The prepared training and validation datasets.\n",
    "\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "        logger.info(\"Loading image datasets from directory: %s\", self.data_dir)\n",
    "        train_dataset, val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "            self.data_dir,\n",
    "            validation_split=self.validation_split,\n",
    "            image_size=self.image_size,\n",
    "            batch_size=self.batch_size,\n",
    "            labels=self.labels,\n",
    "            subset=self.subset,\n",
    "            seed=self.seed,\n",
    "        )\n",
    "        self.class_names = train_dataset.class_names\n",
    "        logger.info(\"Saving class names to file: %s/class_names.txt\", self.save_dir)\n",
    "        with open(self.save_dir + '/class_names.txt', 'w') as f:\n",
    "            f.write('\\n'.join(self.class_names))\n",
    "\n",
    "        logger.info(\"Class names: %s\", self.class_names)\n",
    "        logger.info(\"Prefetching datasets\")\n",
    "        train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "        val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "        logger.info(\"Saving datasets to directory: %s\", self.save_dir)\n",
    "        train_dataset.save(self.save_dir + '/train_dataset')\n",
    "        val_dataset.save(self.save_dir + '/val_dataset')\n",
    "\n",
    "        logger.info(\"Datasets prepared successfully\")\n",
    "        return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainMRI.constants import *\n",
    "from brainMRI.utils.helpers import load_config, create_directories\n",
    "class ConfigHandler:\n",
    "    def __init__(self, file_path=CONFIG_FILE_PATH, params_path = PARAMS_FILE_PATH):\n",
    "        self.config = load_config(file_path)\n",
    "        self.params = load_config(params_path)\n",
    "        create_directories([self.config.root_dir])\n",
    "\n",
    "    \n",
    "    def get_prepare_datasets_config(self) -> PrepareDatasets:\n",
    "        config = self.config.prepare_datasets\n",
    "        params = self.params.prepare_datasets\n",
    "        create_directories([config.save_dir])\n",
    "        \n",
    "        prepare_datasets_config = PrepareDatasets(\n",
    "            data_dir= config.data_dir,\n",
    "            save_dir= config.save_dir,\n",
    "            validation_split= params.validation_split,\n",
    "            image_size= params.image_size,\n",
    "            batch_size= params.batch_size,\n",
    "            labels= params.labels,\n",
    "            subset= params.subset,\n",
    "            seed= params.seed\n",
    "        )\n",
    "\n",
    "        return prepare_datasets_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-01 10:00:31,657: INFO: helpers: YAML file: config\\config.yaml loaded successfully]\n",
      "[2024-06-01 10:00:31,852: INFO: helpers: YAML file: params.yaml loaded successfully]\n",
      "[2024-06-01 10:00:31,856: INFO: helpers: Created directory at: project_outputs]\n",
      "[2024-06-01 10:00:31,858: INFO: helpers: Created directory at: project_outputs/data/preprocesses_data]\n",
      "[2024-06-01 10:00:31,864: INFO: 1209089901: Loading image datasets from directory: project_outputs/data/extracted]\n",
      "Found 253 files belonging to 2 classes.\n",
      "Using 203 files for training.\n",
      "Using 50 files for validation.\n",
      "[2024-06-01 10:00:35,614: INFO: 1209089901: Saving class names to file: project_outputs/data/preprocesses_data/class_names.txt]\n",
      "[2024-06-01 10:00:35,616: INFO: 1209089901: Class names: ['no', 'yes']]\n",
      "[2024-06-01 10:00:35,617: INFO: 1209089901: Prefetching datasets]\n",
      "[2024-06-01 10:00:35,629: INFO: 1209089901: Saving datasets to directory: project_outputs/data/preprocesses_data]\n",
      "[2024-06-01 10:00:44,584: INFO: 1209089901: Datasets prepared successfully]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigHandler()\n",
    "    prepare_datasets_config = config.get_prepare_datasets_config()\n",
    "    prepare_datasets_config.prepare_datasets()\n",
    "except Exception as e:\n",
    "    logger.error(\"Error occurred while preparing datasets: %s\", str(e))\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainMRI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
